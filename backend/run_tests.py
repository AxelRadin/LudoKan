#!/usr/bin/env python
"""
Script pour exÃ©cuter les tests avec diffÃ©rentes configurations
"""
import os
import sys
import subprocess
import argparse
import re


def parse_pytest_output(output):
    """Parse la sortie de pytest pour extraire les statistiques"""
    stats = {
        'passed': 0,
        'failed': 0,
        'skipped': 0,
        'errors': 0,
        'total': 0
    }
    
    # Chercher la ligne de rÃ©sumÃ© (ex: "23 passed, 48 skipped, 2 failed in 12.34s")
    summary_pattern = r'(\d+)\s+passed|(\d+)\s+failed|(\d+)\s+skipped|(\d+)\s+error'
    
    for match in re.finditer(summary_pattern, output):
        if match.group(1):  # passed
            stats['passed'] = int(match.group(1))
        elif match.group(2):  # failed
            stats['failed'] = int(match.group(2))
        elif match.group(3):  # skipped
            stats['skipped'] = int(match.group(3))
        elif match.group(4):  # errors
            stats['errors'] = int(match.group(4))
    
    stats['total'] = stats['passed'] + stats['failed'] + stats['skipped'] + stats['errors']
    
    return stats


def run_command(command, description):
    """ExÃ©cute une commande et affiche le rÃ©sultat"""
    print(f"\n{'='*60}")
    print(f"ğŸ§ª {description}")
    print(f"{'='*60}\n")
    
    # ExÃ©cute la commande en capturant la sortie tout en l'affichant
    process = subprocess.Popen(
        command, 
        shell=True, 
        stdout=subprocess.PIPE, 
        stderr=subprocess.STDOUT,
        text=True,
        bufsize=1
    )
    
    output_lines = []
    for line in process.stdout:
        print(line, end='')
        output_lines.append(line)
    
    process.wait()
    output = ''.join(output_lines)
    
    return process.returncode == 0, output


def main():
    parser = argparse.ArgumentParser(description='ExÃ©cuter les tests LudoKan')
    parser.add_argument('--unit', action='store_true', help='ExÃ©cuter seulement les tests unitaires')
    parser.add_argument('--integration', action='store_true', help='ExÃ©cuter seulement les tests d\'intÃ©gration')
    parser.add_argument('--celery', action='store_true', help='ExÃ©cuter seulement les tests Celery')
    parser.add_argument('--coverage', action='store_true', help='GÃ©nÃ©rer un rapport de couverture')
    parser.add_argument('--verbose', '-v', action='store_true', help='Mode verbeux')
    parser.add_argument('--parallel', '-n', type=int, help='ExÃ©cuter les tests en parallÃ¨le')
    parser.add_argument('--app', help='ExÃ©cuter les tests d\'une app spÃ©cifique')
    
    args = parser.parse_args()
    
    # Message d'introduction
    print("\n" + "="*60)
    print("ğŸ§ª LUDOKAN - SUITE DE TESTS")
    print("="*60)
    print("\nâš ï¸  Note: Les tests des modÃ¨les non implÃ©mentÃ©s seront skipped")
    print("ğŸ“‹ VÃ©rifiez le rÃ©sumÃ© Ã  la fin pour voir le nombre de tests skip\n")
    
    # Configuration de base
    base_command = "python -m pytest"
    
    if args.verbose:
        base_command += " -v"
    
    if args.parallel:
        base_command += f" -n {args.parallel}"
    
    if args.coverage:
        base_command += " --cov=apps --cov-report=html --cov-report=term-missing"
    
    # Filtres de tests
    test_filters = []
    
    if args.unit:
        test_filters.append("-m unit")
    elif args.integration:
        test_filters.append("-m integration")
    elif args.celery:
        test_filters.append("-m celery")
    
    if args.app:
        test_filters.append(f"apps/{args.app}/tests/")
    
    if test_filters:
        base_command += " " + " ".join(test_filters)
    
    # ExÃ©cution des tests
    success, output = run_command(base_command, "ExÃ©cution des tests")
    
    # Parser les rÃ©sultats
    stats = parse_pytest_output(output)
    
    # Affichage du rÃ©sumÃ© final
    print("\n" + "="*60)
    print("ğŸ“Š RÃ‰SUMÃ‰ DES TESTS")
    print("="*60)
    print(f"âœ… Tests rÃ©ussis (PASSED) : {stats['passed']}")
    if stats['failed'] > 0:
        print(f"âŒ Tests Ã©chouÃ©s (FAILED) : {stats['failed']} - Assertions non satisfaites")
    else:
        print(f"âŒ Tests Ã©chouÃ©s (FAILED) : {stats['failed']}")
    
    if stats['errors'] > 0:
        print(f"ğŸ’¥ Erreurs (ERROR)        : {stats['errors']} - Erreurs d'exÃ©cution/setup")
    
    print(f"âš ï¸  Tests skippÃ©s (SKIPPED) : {stats['skipped']}")
    print(f"ğŸ“ˆ Total : {stats['total']} tests")
    print("="*60)
    
    if stats['skipped'] > 0:
        print(f"\nğŸ’¡ {stats['skipped']} test(s) ont Ã©tÃ© skippÃ©s")
        print("   (FonctionnalitÃ©s non encore implÃ©mentÃ©es ou tests Celery dÃ©sactivÃ©s)")
    
    if success and stats['failed'] == 0 and stats['errors'] == 0:
        print(f"\nâœ… Tous les tests actifs passent!")
        if args.coverage:
            print("ğŸ“Š Rapport de couverture gÃ©nÃ©rÃ© dans htmlcov/index.html")
        sys.exit(0)
    else:
        total_issues = stats['failed'] + stats['errors']
        print(f"\nâŒ {total_issues} problÃ¨me(s) dÃ©tectÃ©(s):")
        if stats['failed'] > 0:
            print(f"   â€¢ {stats['failed']} test(s) FAILED (assertions Ã©chouÃ©es)")
        if stats['errors'] > 0:
            print(f"   â€¢ {stats['errors']} ERROR(s) (problÃ¨mes d'exÃ©cution/setup)")
        print("\n   ğŸ“‹ Consultez la sortie ci-dessus pour plus de dÃ©tails")
        sys.exit(1)


if __name__ == "__main__":
    main()
